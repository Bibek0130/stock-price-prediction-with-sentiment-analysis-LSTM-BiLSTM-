# -*- coding: utf-8 -*-
"""Sentiment_analysis_for_stock_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BXvwVrJavcctgNm3ggBrrxpXvL0tUndb

##Sentiment Analysis
It is also called opinion mining. This is where the text is used to make sentiment of the text. THe sentiment can be positive, negative and neutral.

The flow for sentiment Analysis is :

Datasets --> Cleaning and preprocessing --> Choosing algorithm --> constructing our model pipelines --> Evaluations --> predictions

1. Dataset
2. Data Preprocessing
    removing numbers, punctuation marks, special chars and whitespaces
3. Word Embeddings
    using tfidf to convert our text into numerical format.
4. Train models
    Train your models
5. Predictions
    perform predictions and evaluate how model performs

###Data
The data used for this project will be the stock news for nepse data.
"""

#downloading directly from kaggle using opendatasets
##!pip install opendatasets
# import opendatasets as od

# od.download("https://www.kaggle.com/datasets/bibekmainali01/nepse-news-dataset-30k")

import pandas as pd
dataset = "E:/CAB files/7th sem/FYP/Stock Prediction with sentiment Analysis ((LSTM-BiLSTM)/stock_prediction/model/nepse_news_dataset_30k.csv"
#dataset = "nepse_news_dataset_30k.csv"
df = pd.read_csv(dataset)
df.head()

df.tail()

#see how oour data looks
df.shape

#checking for any null values
df.isnull().values.any()
#now we know that we donot have any null values, so we are good to go.

#observing the distriburtion of sentiment in our datasets
import seaborn as sns
df_sample = df.sample(500)  # Take a random sample of 500
sns.countplot(x='News', data=df_sample)
#from the plot we know our data set is uniform.

import seaborn as sns
import matplotlib.pyplot as plt
df_sample = df.sample(500)  # Take a random sample of 500 rows
sns.histplot(df_sample['Label'], bins=3, kde=False)
plt.title('Sentiment Distribution on Sampled Data')
plt.show()

"""##Data Preprocessing

To prepare our model for training, we need to do the following data preprocessing techniques:

1. Data cleaning:
    1. remove unwanted characters
    2. Handle missing values
    3. remove duplicates
2. Encode labels
    convert labels into numerical formats.
    categorical elements like postive into 1 and negative into 0.
4. split the dataset
    split the dataset into trainig and test (80, 20)
5. Tokenization
    change words into numerical indices using tokenizer.
    Because we are using sequential model, tokenizer is used. If we use non-sequential modedl lilke Logistic Regression, NB, SVMm etc we use Vectirization to convert the dataset into numerical formats.

#Data cleaning

1. In cleaning the text, we do the following tasks.
  1. Lower casing
  2. removing HTML tags
  3. removing multiple white spaces
  4. convert to lower case
  5. removing punctuation and numbers
  6. removing single characters
  7. removing stopwords
"""

import re
#downloadinfgthe stoip words from nltk
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
def clean_text(text):
  #re.sub(pattern, replacement, string)
  #text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
  text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
  text = text.lower()  # Convert to lowercase
  #removing puctiation and numbers
  text = re.sub('[^a-zA-Z]',' ',text)
  #removing single characters
  text = re.sub(r"\s+[a-zA-Z]\s+",' ',text)
  #remove stopwords
  pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*')
  text = pattern.sub('',text)
  text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
  return text

df['News'] = df['News'].apply(clean_text)

"""2. Handle missing values"""

df.head()

"""# Encode labels
positive -> 1
Neagtive -> 0
"""

df['Label'].unique()

print(df['Label'].isna())
#verifying that there is no null values in sentiment
print(df['Label'].isna().sum())

from sklearn import preprocessing
Le = preprocessing.LabelEncoder()
df['Label'] = Le.fit_transform(df['Label'])

df['Label'].unique()
df['News'][0]



df.head()

"""# Split the datasets into training and testing
training -> 80%
testing -> 20%
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df['News'], df['Label'], test_size = 0.2, random_state = 42, shuffle=True)

print(X_train.shape)
print(X_test.shape)

"""#Tfidf Vectorization of the text
converts the text into numeri9cal fromat
"""

#convert sparse matrix into a dense array
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features = 5000) #tfidf vectorization with reduced features
tf_x_train = tfidf.fit_transform(X_train)
tf_x_test = tfidf.transform(X_test)

print(tf_x_train.shape)

print(tfidf.vocabulary_)
sum = 0
for i in tfidf.vocabulary_.items():
  print(i)
  sum += 1
print(sum)

"""#prepare data for BiLSTM.
  Since TF-IDF produces sparse vectors, convert them inot sequences and pad them to a fixed length.

#Model building
  we use bilstm model when building
"""

from tensorflow import LSTM, Activation, Dropout, Dense, Input, Embedding, Bidirectional
from tensorflow import Sequential
from tensorflow import Adam

from tensorflow import Sequential
from tensorflow import Bidirectional, LSTM, Dropout, Dense, Input
from tensorflow import Adam

# Define the model
model = Sequential()

# Input layer for sparse matrix compatibility
#model.add(Input(shape=(5000,)))  # Match the `max_features` in TF-IDF
model.add(Input(shape=(tf_x_train.shape[1],)))

# Add dense layers with LSTM-like architecture
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Display the summary
print(model.summary())

# Train the model
history = model.fit(tf_x_train, y_train, batch_size=32, epochs=5, validation_data=(tf_x_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(tf_x_test, y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

"""#predictions"""

# Generate predictions (probabilities)
y_pred_prob = model.predict(tf_x_test)

# Convert probabilities to binary predictions (threshold = 0.5)
y_pred = (y_pred_prob > 0.5).astype(int)

input_News = ''' Kathmandu, Nepal â€“ The Nepal Stock Exchange (NEPSE) index witnessed a significant surge today, gaining 45.32 points to close at 2,105.67, marking a 2.2% increase from the previous trading session. The rally was fueled by strong investor confidence, particularly in the banking and hydropower sectors.

The commercial banking sector saw a sharp rise, with stocks of Nabil Bank (NABIL) and Nepal Investment Bank (NIBL) increasing by 4.5% and 3.8%, respectively. Similarly, the hydropower sector benefitted from government announcements regarding new hydropower projects and favorable energy policies, pushing stocks of Butwal Power Company (BPCL) and Upper Tamakoshi Hydropower (UPPER) up by 5.1% and 6.3%, respectively.

Analysts attribute the bullish trend to positive economic indicators, an upcoming interest rate cut by Nepal Rastra Bank, and increased foreign investment inflows. However, experts caution that profit booking in the next few sessions could lead to minor corrections.
'''
# Preprocess the News
cleaned_News = clean_text(input_News)

# Transform using TF-IDF
transformed_News = tfidf.transform([cleaned_News])
# Predict sentiment
prediction = model.predict(transformed_News)
Labels = "Positive" if prediction[0] > 0.5 else "Negative"

print(f"Predicted Sentiment: {Labels}")

from sklearn.metrics import accuracy_score, roc_auc_score
#accuracy score of the predicteed values
print(accuracy_score(y_test, y_pred))

#roc_auc_score
print(roc_auc_score(y_test, y_pred))

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# Print a few sample predictions
print("First 10 Test Labels: ", y_test[:10].tolist())
print("First 10 Predictions: ", y_pred[:10].tolist())

# Check class distribution
unique, counts = np.unique(y_test, return_counts=True)
print("Class Distribution in Test Set:", dict(zip(unique, counts)))

# Print Confusion Matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Print Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred))